{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nothing\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import requests, json, sys, time, re\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "\n",
    "#URL = 'https://twitter.com/i/search/timeline?f=tweets&vertical=default&q=google%20OR%20Google%20OR%20%23google%20OR%20%23Google%20OR%20%24GOOG%20OR%20GOOGLE%20OR%20%23GOOGLE%20lang%3Aen%20since%3A2014-01-01%20until%3A2014-01-30&src=typd&include_available_features=1&include_entities=1&lang=en'\n",
    "\n",
    "#r = requests. get(URL)\n",
    "#t = r.text\n",
    "\n",
    "#f = open('scrape_first_page','w')\n",
    "#f.write(str(t))\n",
    "#f.close()\n",
    "\n",
    "with open (\"scrape_first_page\", \"r\") as myfile:\n",
    "    d=myfile.read().replace('\\n', '')\n",
    "\n",
    "data = json. loads(d)\n",
    "\n",
    "item_html=data['items_html']\n",
    "\n",
    "pprint(item_html)\n",
    "\n",
    "print (len(item_html))\n",
    "\n",
    "html_string = json. dumps(item_html)\n",
    "soup = BeautifulSoup(html_string, 'html.parser')\n",
    "\n",
    "for tweet in soup.findAll('p'):\n",
    "    print (tweet.text)\n",
    "    print ('\\n')\n",
    "\"\"\"\n",
    "print (\"nothing\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://twitter.com/i/search/timeline?f=tweets&vertical=default&q=google%20OR%20$GOOG%20lang%3Aen%20since%3A2014-01-01%20until%3A2014-01-02&src=typd&include_available_features=1&include_entities=1&lang=en\n",
      "1 418532083196514304 [2014-01-01 23:59:59] - Google planning something big with Boston Dynamics - http://www.pingmagazine.tv/news/google-planning-something-big-boston-dynamics/ … pic.twitter.com/mc4AWsdiUE\n",
      "2 418532075294429184 [2014-01-01 23:59:57] - Google planning something big with Boston Dynamics: Boston Dynamics: Now Belongs to Google… http://goo.gl/fb/1NJTV \n",
      "3 418532066767413248 [2014-01-01 23:59:55] - (Ps sorry these aren't mine,me and google are bffs ;3)\n",
      "4 418532062312689666 [2014-01-01 23:59:54] - If i had to guess, i would guess that the number one search word on Bing is Google.\n",
      "5 418532059863212032 [2014-01-01 23:59:53] - @troythedecoy @chrisprodromou @muswellhillspur @louise_s_page it goes back years mate, google it.\n",
      "6 418532054653865984 [2014-01-01 23:59:52] - Wahhahhaa Rt \"@dailyteenwords: When I do homework: Books=NO, Library =NO, Google = YES.\"\n",
      "7 418532048236605441 [2014-01-01 23:59:50] - How Google Analytics Dashboards Can Make Your Life Easier http://kiss.ly/1ePzZMM \n",
      "8 418532041324789760 [2014-01-01 23:59:49] - #KnowYourAnon I'm a virgin but no one long distance believes that because i can sext like a mother fucker thanks to google \n",
      "9 418532040238063616 [2014-01-01 23:59:49] - Most beneficial Google Plus Groups For Your Marketing Efforts http://nblo.gs/Su7h7 \n",
      "10 418532016527650816 [2014-01-01 23:59:43] - @kordinglab Look: \"Your profile doesn't include a verified email and won't appear in Google Scholar search\"\n",
      "11 418532015257161728 [2014-01-01 23:59:43] - @westhamster2 Google, if l was a lesbian l'd want Dido Armstrong as my dear wife  xxx\n",
      "12 418532008449425408 [2014-01-01 23:59:41] - Anybody up for a google hangout it might be only 45 mins though\n",
      "13 418532004968161280 [2014-01-01 23:59:40] - @Pandarama_ right lemme finish game then im gonna google map cause i dunno these ends\n",
      "14 418532003051761664 [2014-01-01 23:59:40] - Idk i got so bored and i was just searching random things into google and i seriously managed to get onto \" what is the meaning of life\"\n",
      "15 418531999591448577 [2014-01-01 23:59:39] - Mom: \"I should work for google. They have snacks and Lego heads.\"\n",
      "16 418531998164975616 [2014-01-01 23:59:38] - Most beneficial Google Plus Groups For Your Marketing Efforts http://fb.me/1O9TiRbai \n",
      "17 418531992939286528 [2014-01-01 23:59:37] - Google services AGAIN. pic.twitter.com/VQwRNvPUo3\n",
      "18 418531991957798912 [2014-01-01 23:59:37] - Good article: Getting things done with Google Tasks #internet http://www.mattcutts.com/blog/todo-list-tips/ …\n",
      "19 418531978518855681 [2014-01-01 23:59:34] - \"Typing song lyrics into Google just to find out the name of the song.\"\n",
      "20 418531972974391297 [2014-01-01 23:59:32] - Oh nice, buy it then\"@Dat_fair_boi: Daabi Tonaton RT @otomatiks: Google search  anaa?\"Dat_fair_boi: Have Found Love\"\"\n",
      "https://twitter.com/i/search/timeline?f=tweets&vertical=default&q=google%20OR%20$GOOG%20lang%3Aen%20since%3A2014-01-01%20until%3A2014-01-02&src=typd&include_available_features=1&include_entities=1&lang=en&max_position=TWEET-418531972974391297-418532083196514304-BD1UO2FFu9QAAAAAAAAETAAAAAcAAAASAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA&reset_error_state=false\n",
      "'TWEET-418531876353998848-418532083196514304-BD1UO2FFu9QAAAAAAAAETAAAAAcAAAASAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA'\n",
      "21 418531962366603264 [2014-01-01 23:59:30] - #News A New Year's Day dip: Hundreds dive into ocean during polar bear plunge in ... - The Star-L... http://bit.ly/1hgKYk2  Vía @Google\n",
      "22 418531961800388608 [2014-01-01 23:59:30] - #News Former First Lady Barbara Bush Hospitalized - New Philadelphia Times Reporter http://bit.ly/19JZNXD  Vía @Google\n",
      "23 418531960646926336 [2014-01-01 23:59:30] - #News Building explosion in Minneapolis injures 14 people, 6 critically, authorities say - Fox News http://bit.ly/1gn9T2n  Vía @Google\n",
      "24 418531958466293760 [2014-01-01 23:59:29] - Google Glass won't succeed as a consumer product in 2014 due to high expectations, cost, more » http://bit.ly/KjNCaY  #tech\n",
      "25 418531956343996416 [2014-01-01 23:59:29] - With the help of google I have diagnosed myself with insomnia\n",
      "26 418531956151029760 [2014-01-01 23:59:28] - New Event Now On. GET Rank AAA Monsters. App Store: http://bit.ly/AzhyKP  Google Play: http://bit.ly/NtcNTs  #DarkSummoner\n",
      "27 418531951520526336 [2014-01-01 23:59:27] - Talk I'm telling you this is getting big. Last week we had Toni Woods google him. We got a special… http://instagram.com/p/ipUKRkGsyr/ \n",
      "28 418531944796663809 [2014-01-01 23:59:26] - Learn HOW to Build a Website in WordPress, That will get Google's Attention - http://eepurl.com/LBx9z \n",
      "29 418531944490889217 [2014-01-01 23:59:26] - A Sensible Health Care System for All Americans - Google Drive http://ow.ly/sc84m \n",
      "30 418531937540931585 [2014-01-01 23:59:24] - @Zadddy_Dirah @MoonTooCool we saw you on google you was looking HIT \n",
      "31 418531937104302080 [2014-01-01 23:59:24] - \"Hey google image SHUT THE F*CK UP.\" -dad\n",
      "32 418531925989797888 [2014-01-01 23:59:21] - @jammmiexo google what first period is tomorrow\n",
      "33 418531911368069120 [2014-01-01 23:59:18] - WHY IS GOOGLE CHROME SO FUCKING SHIT\n",
      "34 418531903885828096 [2014-01-01 23:59:16] - You don't know something? Google it. You don't know someone? Facebook it. You don't find something? MOM!\n",
      "35 418531899179798529 [2014-01-01 23:59:15] - Bump bumped-off as Google quickly shuts down file-sharing acquisition - http://www.appy-geek.com/Web/ArticleWeb.aspx?regionid=1&articleid=17248051 …\n",
      "36 418531896071815168 [2014-01-01 23:59:14] - Blogging Income & Expenses Spreadsheet - Free Google Drive download http://pinterest.com/pin/3799980909480888/ …\n",
      "37 418531885652795392 [2014-01-01 23:59:12] - How Google and Apple are turning your car into a giant smartphone http://www.nbcnews.com/technology/how-google-apple-are-turning-your-car-giant-smartphone-2D11820932 …\n",
      "38 418531881915645952 [2014-01-01 23:59:11] - Don't mess with #google. RT @zaibatsu: One chart shows Google's awesome power to kill your website's traffic http://zite.to/1cmCK1v \n",
      "39 418531879520718848 [2014-01-01 23:59:10] - @Kirsty_Henshaw just done a quick google it could well help. Tai chi and Chi Gung builds strength but are lots gentler! Acupuncture too\n",
      "40 418531876353998848 [2014-01-01 23:59:09] - Google-owned Bump and Flock announce January 31 http://www.regrit.com/google-owned-bump-and-flock-announce-january-31st-shutdown-app-store-removal/ … #App #appstore #Apps #bump #CEO #data #DavidLieb #Flock #good #g\n",
      "https://twitter.com/i/search/timeline?f=tweets&vertical=default&q=google%20OR%20$GOOG%20lang%3Aen%20since%3A2014-01-01%20until%3A2014-01-02&src=typd&include_available_features=1&include_entities=1&lang=en&max_position=TWEET-418531876353998848-418532083196514304-BD1UO2FFu9QAAAAAAAAETAAAAAcAAAASAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA&reset_error_state=false\n"
     ]
    }
   ],
   "source": [
    "import requests, json, sys, time, re\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "\n",
    "counter = 0\n",
    "max_tweets = 40\n",
    "rate_delay = 5\n",
    "\n",
    "\n",
    "def build_url(since, until, query, max_position=None):\n",
    "    space = '%20OR%20'\n",
    "    q = space.join(query)\n",
    "    s = '-'.join(since)\n",
    "    u = '-'.join(until)\n",
    "    s = '%20lang%3Aen%20since%3A'+s\n",
    "    u = '%20until%3A'+u\n",
    "\n",
    "    start = 'https://twitter.com/i/search/timeline?f=tweets&vertical=default&q='        \n",
    "    end   = '&src=typd&include_available_features=1&include_entities=1&lang=en'\n",
    "    if max_position is not None:\n",
    "        URL   = start+q+s+u+end+max_position\n",
    "    else:\n",
    "        URL   = start+q+s+u+end\n",
    "    \n",
    "    return URL\n",
    "    \n",
    "\n",
    "\n",
    "def execute_search(url):\n",
    "    \"\"\"\n",
    "    Executes a search to Twitter for the given URL\n",
    "    :param url: URL to search twitter with\n",
    "    :return: A JSON object with data from Twitter\n",
    "    \"\"\"\n",
    "    print(url)\n",
    "    response = requests. get(url)  \n",
    "    data = json. loads(response.text)\n",
    "    return data\n",
    "\n",
    "def parse_tweets(items_html):\n",
    "    \"\"\"\n",
    "    Parses Tweets from the given HTML\n",
    "    :param items_html: The HTML block with tweets\n",
    "    :return: A JSON list of tweets\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(items_html)\n",
    "    tweets = []\n",
    "    for li in soup.find_all(\"li\", class_='js-stream-item'):\n",
    "\n",
    "        # If our li doesn't have a tweet-id, we skip it as it's not going to be a tweet.\n",
    "        if 'data-item-id' not in li.attrs:\n",
    "            continue\n",
    "\n",
    "        tweet = {\n",
    "            'tweet_id': li['data-item-id'],\n",
    "            'text': None,\n",
    "            'user_id': None,\n",
    "            'user_screen_name': None,\n",
    "            'user_name': None,\n",
    "            'created_at': None,\n",
    "            'retweets': 0,\n",
    "            'favorites': 0\n",
    "        }\n",
    "\n",
    "        # Tweet Text\n",
    "        text_p = li.find(\"p\", class_=\"tweet-text\")\n",
    "        if text_p is not None:\n",
    "            tweet['text'] = text_p.get_text()\n",
    "\n",
    "        # Tweet User ID, User Screen Name, User Name\n",
    "        user_details_div = li.find(\"div\", class_=\"tweet\")\n",
    "        if user_details_div is not None:\n",
    "            tweet['user_id'] = user_details_div['data-user-id']\n",
    "            tweet['user_screen_name'] = user_details_div['data-user-id']\n",
    "            tweet['user_name'] = user_details_div['data-name']\n",
    "\n",
    "        # Tweet date\n",
    "        date_span = li.find(\"span\", class_=\"_timestamp\")\n",
    "        if date_span is not None:\n",
    "            tweet['created_at'] = float(date_span['data-time-ms'])\n",
    "\n",
    "        # Tweet Retweets\n",
    "        retweet_span = li.select(\"span.ProfileTweet-action--retweet > span.ProfileTweet-actionCount\")\n",
    "        if retweet_span is not None and len(retweet_span) > 0:\n",
    "            tweet['retweets'] = int(retweet_span[0]['data-tweet-stat-count'])\n",
    "\n",
    "        # Tweet Favourites\n",
    "        favorite_span = li.select(\"span.ProfileTweet-action--favorite > span.ProfileTweet-actionCount\")\n",
    "        if favorite_span is not None and len(retweet_span) > 0:\n",
    "            tweet['favorites'] = int(favorite_span[0]['data-tweet-stat-count'])\n",
    "\n",
    "        tweets.append(tweet)\n",
    "    return tweets\n",
    "\n",
    "def save_tweets(tweets):\n",
    "    \"\"\"\n",
    "    Just prints out tweets\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global counter\n",
    "    global max_tweets\n",
    "    for tweet in tweets:\n",
    "        # Lets add a counter so we only collect a max number of tweets\n",
    "        counter += 1\n",
    "\n",
    "        if tweet['created_at'] is not None:\n",
    "            t = datetime.datetime.fromtimestamp((tweet['created_at']/1000))\n",
    "            fmt = \"%Y-%m-%d %H:%M:%S\"\n",
    "            #print \"%i [%s] - %s\" % (counter, t.strftime(fmt), tweet['text'])\n",
    "            print (str(counter) + ' ' + str(tweet['tweet_id']) + ' [' + str(t.strftime(fmt)) + '] - ' + str(tweet['text']))\n",
    "            \n",
    "        # When we've reached our max limit, return False so collection stops\n",
    "        if counter >= max_tweets:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "    \n",
    "def search(since, until, query):\n",
    "    first = True\n",
    "    global rate_delay\n",
    "    url = build_url(since, until, query)\n",
    "    continue_search = True\n",
    "    min_tweet = None\n",
    "    response = execute_search(url)\n",
    "    while response is not None and continue_search and response['items_html'] is not None:\n",
    "        tweets = parse_tweets(response['items_html'])\n",
    "        if first:\n",
    "            first = False\n",
    "        else:   \n",
    "            position = response['min_position']\n",
    "            pprint(position)\n",
    "\n",
    "        # If we have no tweets, then we can break the loop early\n",
    "        if len(tweets) == 0:\n",
    "            break\n",
    "\n",
    "        # If we haven't set our min tweet yet, set it now\n",
    "        if min_tweet is None:\n",
    "            min_tweet = tweets[0]\n",
    "\n",
    "        continue_search = save_tweets(tweets)\n",
    "\n",
    "        # Our max tweet is the last tweet in the list\n",
    "        max_tweet = tweets[-1]\n",
    "        if min_tweet['tweet_id'] is not max_tweet['tweet_id']:\n",
    "            max_position = \"&max_position=TWEET-%s-%s-BD1UO2FFu9QAAAAAAAAETAAAAAcAAAASAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA&reset_error_state=false\" % (max_tweet['tweet_id'], min_tweet['tweet_id'])\n",
    "            url = build_url(since, until, query, max_position=max_position)\n",
    "            #Sleep for our rate_delay\n",
    "            sleep(rate_delay)\n",
    "            response = execute_search(url)\n",
    "    \n",
    "    \n",
    "query = ['google', '$GOOG']\n",
    "since = ['2014', '01', '01']\n",
    "until = ['2014', '01', '02']\n",
    "search(since, until, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
